\documentclass[a4paper, 10pt, ngerman, fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{ngerman}
\usepackage{coordsys,logsys,color}
\usepackage{german,fancyhdr}
\usepackage{hyperref}
\usepackage{texdraw}
\usepackage{expdlist}
\usepackage{graphicx} %für Bilder
\usepackage{tabularx}
\usepackage{cite}
\usepackage{pdfpages}
\usepackage{geometry}
\geometry{a4paper,left=35mm,right=35mm, top=3cm, bottom=3cm} 

\input{txdtools}


\NeedsTeXFormat{LaTeX2e}
\ProvidesPackage{hyperref}
\definecolor{darkblue}{rgb}{0,0,.6}
\hypersetup{pdftex=false, colorlinks=true, breaklinks=true, linkcolor=black, menucolor=darkblue, pagecolor=darkblue, urlcolor=darkblue, citecolor=darkblue}




\begin{document}

% ACHTUNG
% Titelseite ist STATIC !!!

%\includepdf{./titelseite.pdf}
\setcounter{page}{1}
% für Inhaltsverzeichnis nächste Zeile EIN / AUS:
%\tableofcontents
%\newpage

\section{Ausgangslage}
Im Rahmen des Semesterkurzbeitrags IE1 werden 50 Anfragen auf einer Kollektion von über 20‘000 Dokumenten ausgewertet. Die Anfragen stellen Informationsbedürfnisse dar, welche manchmal in Stichworten, aber auch als vollständige Sätze formuliert sind. Es wird mittels eines Experiments Information Retrieval (IR)-Systeme auf ihre Fähigkeit hin untersucht, diese Informationsbedürfnisse mittels relevanter Information zu \glqq{}erfüllen\grqq{}. 

\section{Forschungsfrage / Hypothese}
Basierend auf die von den Autoren in F-Sharp implementierte Information-Retrieval-Appli- kation wird folgende Forschungsfrage untersucht: \glqq{}Ändert sich die Reihenfolge der Resultate  hinsichtlich des RSV, wenn die vorgegebene Query vorgängig aufbereiten werden? Befinden sich auf den obereren Rängen zutreffendere Resultate?\grqq{}


\section{Vorgehen}
\subsection{Technische Implementation}
In der ersten Phase haben wir uns Gedanken zur Implementierung gemacht. Schlussendlich haben wir uns für eine funktionale Lösung mit F-Sharp entschieden. Wir vermuteten, dass wir mit F-Sharp eine gute Performance erreichen. Durch die funktionale Programmierung bekommen wir die Parallelität quasi geschenkt. Nach der herausfordernder Implementierungsphase erhielten wir eine performante Lösung. Mit Hilfe der Resultate aus dem Praktikum 5 haben wir unsere Applikation verifiziert. Hierzu haben wir die Testdaten (Queries und Documents) des Praktikum 5 unserer Applikation übergeben und die erhaltene Rangliste mit der Musterlösung verglichen. Wir fanden  unsere Top-Antworten ebenso im oberen Bereich der Musterlösung und gehen somit davon aus, dass die F-Sharp-Applikation zuverlässige Werte liefert. \\
Unsere Applikation durchläuft die folgenden Prozesse:
einlesen Dokumentenkollektion und Stamming, Erstellung Inverted- und Non-Inverted-Index, Kalkullierung IDF, D-Norm Generierung, einlesen Queyries, Generierung Q-Norm und Akkumulator, Berechnung RSV und Ausgabe in TREC-Format.\\
Das Coderpository finden Sie auf \url{https://github.com/hedigerf/ie_semesterkurzbeitrag}. 
Das Programm spielt sich hauptsächlich in den Files Indexes.fs und Program.fs ab.


\subsection{Query Anreicherungsprozess}
Jedes Query wird von Hand bearbeitet. Im ersten Schritt werden repetitive Wörter und Füllwörter entfernt. Anschliessend wird dieses Zwischenquery der Suchmaschine google.com übergeben. Von den Suchergebnissen werden Schlüsselwörter extrahiert und dem Zwischenquery ergänzt.\\
Die neu definierte Abfrage wird dem F-Sharp Retrieval Applikation abgesetzt. Die neu entstandene Rangliste wird mit der ursprünglichen Rangliste verglichen und bezüglich der Forschungsfrage analysiert.
Nachfolgend das Query mit der recordId 245, welches als Beispiel für eine abgeänderten Abfrage hinhält.\\

\begin{description}[\setlabelstyle{\bfseries}\setleftmargin{9em}]
	\item[original]
	transistor phase splitting circuits
	
	\item[angereichert]
	transistor phase splitting circuits two-stage single-stage splitter push pull amplifier drivers
\end{description}
 
\section{Resultate}
Die zwei erhaltenen Ranglisten mit den Resultaten zu jedem Query werden in diesem Kapitel analysiert. Dazu werden die Ergebnisse auf dem ersten Rang verglichen. Die Resultate werden auf das Informationsbedürfnis der original Query hin untersucht.


\subsection{Analyse / Gegenüberstellung}
Es wurden die fünf zufällig selektierten Suchanfragen ausgewertet:\\
\textbf{37845} Auf dem 1. Rang stand das gleiche Dokument 19975. Somit wurde das zweit beste Ergebnis untersucht. Der Eintrag 21387 des Originals ist besser, da hier wenigstens ein Teil des Queries erfüllt ist (lunar surface).\\
\textbf{14536} Dieses Query ist eine Abfrage, die in Prosa Form verfasst wurde. Das Ergebnis des Originals enthält die Rollen der Bibliotheken in Skandinavien, welche Probleme von Entwicklungsländern behandeln. Die erweiterte Version liefert einen Eintrag zu electron publishing, jedoch wird das Thema Entwicklungsländer nicht berücksichtigt. Es gibt hier keinen klaren Sieger. Beide Resultate sind mässig befriedigend.\\
\textbf{2405} Nochmals liefern die zwei Queries das gleiche Resultat auf dem 1. Rang. Das Original gewinnt schlussendlich, da beim Resultat zumindest das Pagging im Ergebnis behandelt wird.\\
\textbf{377026} Das Resultat des Originals ist ein Volltreffer. Die Erweiterte Query liefert ein sehr schlechtes Ergebnis und verfehlt die Anfrage.\\
\textbf{1217} Das Original liefert ein sehr schlechtes Ergebnis, welches uns nicht weiterbringt. Das Ergebnis der erweiterten Abfrage tangiert zumindest das Thema. 

\subsection{Übersicht Ergebnis}
In der nachfolgenden Tabelle werden die Ergebnisse der Stichprobe dargestellt. In der ersten Spalte steht die Query-Nummer. Danach folgt das Resultat der Original Query mit RSV. In der vierten und fünften Spalten ist das Resultat von der abgeänderten Abfrage mit RSV. Am Schluss das Resultat vom Vergleich zwischen original und der angereichten Version.\\
\\
\begin{tabular}{|l|l|l|l|l|l|l|}\hline\hline
	\textbf{QueryNr.} & \textbf{Original} & \textbf{Org.RSV} & \textbf{Erweitert} & \textbf{Erw.RSV} & \textbf{Sieger} \\ \hline
	37845  & 19975 & 0.586 & 19975 & 0.427 &      \\ \hline
	37845  & 21387 & 0.497 & 6624  & 0.353 & Org. \\ \hline
	14536  & 7567  & 1.019 & 4314  & 0.215 & ---  \\ \hline
	2405   & 1572  & 0.493 & 1572  & 0.764 &      \\ \hline
	2405   & 22529 & 0.429 & 35689 & 0.602 & Org. \\ \hline
	377026 & 3704  & 0.585 & 18141 & 0.367 & Org. \\ \hline
	1217   & 25913 & 0.282 & 24224 & 0.285 & Erw. \\ \hline\hline
\end{tabular}

\section{Diskussion/Konklusion}
Die Ergebnisse der Analyse zeigen, dass die original Queries bessere Antworten liefern, anstelle der erweiterten Abfragen.
Bei den fünf untersuchten Abfragen mit dem original Text gab es ein schlechtes  Ergebnis und ein unbefriedigendes. Die angereicherten Abfragen haben die gleiche Fehlerquote. Hingegen waren die Ergebnisse weniger zutreffend verglichen mit der original Abfrage. Des weiteren sind bei zwei von fünf Abfragen das gleiche Ergebnis auf dem ersten Rang. Dies bedeutet, dass die Erweiterung auf das Top Ergebnis keinen Einfluss hatte.\\
Unser mini Projekt hat aufgezeigt, dass es sich nicht lohnt eine Website anzubieten, die dem Benutzer von Menschenhand hilft bei der Suche. Gemeint ist, dass die Suchabfrage erweitert wird und ein besseres Ergebnis retournieren. Ernüchternd!

\end{document}


%\begin{tabular}{|l|l|l|l|l|l|l|}\hline\hline
%	\textbf{Spalte1} & \textbf{Spalte2} & \textbf{Spalte3} & \textbf{Spalte4} & \textbf{Spalte5} & \textbf{Spalte6} & \textbf{Spalte7}\\ \hline
%	1 & 2 & 3 & 4 & 5 & 6 & 7\\ \hline
%	1 & 2 & 3 & 4 & 5 & 6 & 7\\ \hline\hline
%\end{tabular}